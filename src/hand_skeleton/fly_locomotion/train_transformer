import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

from tensorflow.keras import layers, models
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical

# =========================================================
# 1) Load dataset (same format as your txt)
#    line: "<label_str> f1 f2 ... f168"
#    label_str: rotation -> class 0, translation/else -> class 1
# =========================================================
txt_path = "/home/hoon/hand_skeleton/skeleton_dataset/train/train_241201.txt"

skeleton_data = []
labels_int = []
pointing = 0
none = 0

with open(txt_path, "r") as f:
    for line in f.readlines():
        data = line.split()
        skeleton_data.append(data[1:])

        if data[0] == "translation":
            labels_int.append(1)  # class 1
            none += 1
        elif data[0] == "rotation":
            labels_int.append(0)  # class 0
            pointing += 1
        else:
            labels_int.append(1)  # class 1
            none += 1

X = np.array(skeleton_data, dtype=np.float32)
y = np.array(labels_int, dtype=np.int64)

print("X:", X.shape, "y:", y.shape, "pointing/none:", pointing, none)

num_classes = 2
y_onehot = to_categorical(y, num_classes=num_classes)

# =========================================================
# 2) Train/Val split
#    (논문/리뷰 대응 기준으로는 전체 평가 말고 val/test만 봐야 함)
# =========================================================
X_train, X_val, y_train, y_val, y_train_int, y_val_int = train_test_split(
    X, y_onehot, y, test_size=0.2, random_state=42, stratify=y
)

# =========================================================
# 3) Reshape: (N, 168) -> (N, J, C)
#    Here: C=3 (xyz), J=168/3=56
# =========================================================
input_dim = X_train.shape[1]
if input_dim % 3 != 0:
    raise ValueError(f"input_dim={input_dim} is not divisible by 3. Can't reshape to (J,3).")

J = input_dim // 3  # number of joint tokens
C = 3               # xyz

X_train = X_train.reshape(-1, J, C)
X_val   = X_val.reshape(-1, J, C)

print("Reshaped X_train:", X_train.shape, "X_val:", X_val.shape)

# =========================================================
# 4) Transformer Encoder block
# =========================================================
@tf.keras.utils.register_keras_serializable()
class TransformerEncoder(layers.Layer):
    def __init__(self, d_model: int, num_heads: int, dff: int, dropout: float = 0.1, **kwargs):
        super().__init__(**kwargs)

        self.d_model = d_model
        self.num_heads = num_heads
        self.dff = dff
        self.dropout = dropout

        self.mha = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)
        self.ffn = models.Sequential([
            layers.Dense(dff, activation="relu"),
            layers.Dense(d_model),
        ])
        self.norm1 = layers.LayerNormalization(epsilon=1e-6)
        self.norm2 = layers.LayerNormalization(epsilon=1e-6)
        self.drop1 = layers.Dropout(dropout)
        self.drop2 = layers.Dropout(dropout)

    def call(self, x, training=False):
        attn_out = self.mha(x, x, training=training)
        attn_out = self.drop1(attn_out, training=training)
        x = self.norm1(x + attn_out)

        ffn_out = self.ffn(x, training=training)
        ffn_out = self.drop2(ffn_out, training=training)
        return self.norm2(x + ffn_out)

    def get_config(self):
        config = super().get_config()
        config.update({
            "d_model": self.d_model,
            "num_heads": self.num_heads,
            "dff": self.dff,
            "dropout": self.dropout,
        })
        return config

# =========================================================
# 5) Build Joint-Transformer (tokens = joints)
# =========================================================
def build_joint_transformer(
    num_classes: int,
    J: int,
    C: int,
    d_model: int = 64,
    num_heads: int = 4,
    dff: int = 128,
    num_layers: int = 2,
    dropout: float = 0.15,
) -> tf.keras.Model:
    inp = layers.Input(shape=(J, C))               # (56,3)

    # xyz -> embedding
    x = layers.Dense(d_model)(inp)

    # learnable positional embedding (joint index)
    pos_emb = layers.Embedding(input_dim=J, output_dim=d_model)
    positions = tf.range(start=0, limit=J, delta=1)
    x = x + pos_emb(positions)

    x = layers.Dropout(dropout)(x)

    for _ in range(num_layers):
        x = TransformerEncoder(d_model=d_model, num_heads=num_heads, dff=dff, dropout=dropout)(x)

    # pooling for classification
    x = layers.GlobalAveragePooling1D()(x)
    x = layers.Dropout(dropout)(x)
    x = layers.Dense(64, activation="relu")(x)
    x = layers.Dropout(dropout)(x)

    out = layers.Dense(num_classes, activation="softmax")(x)

    model = models.Model(inputs=inp, outputs=out, name="joint_transformer")
    model.compile(
        optimizer="adam",
        loss="categorical_crossentropy",
        metrics=["accuracy"],
    )
    return model

model = build_joint_transformer(
    num_classes=num_classes,
    J=J,
    C=C,
    d_model=64,
    num_heads=4,
    dff=128,
    num_layers=2,
    dropout=0.15,
)

model.summary()

# =========================================================
# 6) Train
# =========================================================
early_stopping = EarlyStopping(monitor="val_loss", patience=15, restore_best_weights=True)

history = model.fit(
    X_train,
    y_train,
    validation_data=(X_val, y_val),
    epochs=100,
    batch_size=32,
    # callbacks=[early_stopping],
)

# =========================================================
# 7) Evaluate on validation set (recommended)
# =========================================================
val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)
print(f"[Transformer] Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

pred = model.predict(X_val, verbose=0)
pred_cls = np.argmax(pred, axis=1)

acc = accuracy_score(y_val_int, pred_cls)
prec = precision_score(y_val_int, pred_cls, average="weighted")
rec = recall_score(y_val_int, pred_cls, average="weighted")
f1 = f1_score(y_val_int, pred_cls, average="weighted")
print(f"[Transformer] Acc={acc:.4f} Prec={prec:.4f} Rec={rec:.4f} F1={f1:.4f}")

# =========================================================
# 8) Plot loss curve
# =========================================================
plt.figure(figsize=(10, 6))
plt.plot(history.history["loss"], label="Loss")
plt.plot(history.history["val_loss"], label="Validation Loss")
plt.xlabel("Epochs", fontsize=12, fontname="Times New Roman")
plt.ylabel("Loss", fontsize=12, fontname="Times New Roman")
plt.grid()
# plt.savefig("/home/hoon/hand_skeleton/training_loss_plot_transformer.png", dpi=300, bbox_inches="tight")
plt.show()

# Optional save
model.save("/home/hoon/hand_skeleton/fly_locomotion/model/model_transformer.keras")
